{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80155183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ba83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c10e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Transformers are powerful sequence models.\"\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(\"Token IDs:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341356a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(tokens)\n",
    "print(\"Decoded text:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada0d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello world!\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
